# MiniCompiler

Лёгкая реализация компилятора для языка, похожего на C, с поддержкой лексического анализа (токенизации), обработки ошибок и набором тестов.

## Команда

- Индивидуальный проект (выполнен в рамках курса по конструированию компиляторов)

## Возможности

- Лексический анализ (токенизация) для C-подобного языка
- Поддержка ключевых слов, идентификаторов, литералов, операторов и разделителей
- Точное отслеживание строк и столбцов
- Комплексная обработка ошибок с восстановлением после них
- Поддержка UTF-8
- Временная сложность O(n) с эффективным использованием памяти

## Спецификация языка

Полная спецификация языка доступна в файле [`docs/language_spec.md`](docs/language_spec.md) и включает:

- Формальное EBNF-определение грамматики
- Категории токенов с регулярными выражениями
- Ключевые слова: `if`, `else`, `while`, `for`, `int`, `float`, `bool`, `return`, `true`, `false`, `void`, `struct`, `fn`
- Идентификаторы: буквы, цифры, подчёркивания (макс. 255 символов, регистрозависимые)
- Литералы: целые числа, числа с плавающей точкой, строки, булевы значения
- Операторы: арифметические (`+ - * / %`), реляционные (`== != < <= > >=`), логические (`&& || !`)
- Разделители: `() {} [] ; ,`
- Комментарии: однострочные (`//`) и многострочные (`/* */`) с опциональной вложенностью
- Пробельные символы: пробел, табуляция, перевод строки (`\n`), возврат каретки (`\r`)

## Структура проекта

```
compiler-project/
├── src/
│   ├── lexer/
│   │   ├── __init__.py
│   │   ├── token.py          # Определения токенов и перечисления
│   │   ├── scanner.py        # Основная реализация сканера
│   │   └── errors.py         # Обработка ошибок
│   └── utils/
│       ├── __init__.py
│       └── helpers.py         # Вспомогательные функции
├── tests/
│   ├── lexer/
│   │   ├── valid/             # Тесты с корректным вводом (20+ тестов)
│   │   └── invalid/           # Тесты с некорректным вводом (10+ тестов)
│   └── test_runner.py         # Автоматический запуск тестов
├── examples/
│   └── hello.src               # Пример исходного файла
├── docs/
│   └── language_spec.md        # Формальная спецификация языка
├── README.md
├── pyproject.toml              # Конфигурация проекта
└── setup.py                     # Скрипт установки
```

## Инструкция по сборке

### Требования

- Python 3.8 или выше
- pip (менеджер пакетов Python)

### Установка

```bash
# Клонирование репозитория
git clone <repository-url>
cd compiler-project

# Установка в режиме разработки
pip install -e .

# Или установка с зависимостями для тестирования
pip install -e .[test]
```

### Примечания для разных платформ

**Linux/macOS:**
```bash
# Создание и активация виртуального окружения (опционально, но рекомендуется)
python3 -m venv venv
source venv/bin/activate
pip install -e .
```

**Windows:**
```bash
# Создание и активация виртуального окружения (опционально, но рекомендуется)
python -m venv venv
venv\Scripts\activate
pip install -e .
```

## Быстрый старт

### Базовое использование

```bash
# Запуск лексера на исходном файле
python -m src.lexer.scanner --input examples/hello.src --output tokens.txt

# Или через консольную команду (если настроено)
compiler lex --input examples/hello.src --output tokens.txt

# Пример вывода (tokens.txt):
# 1:1 KW_FN "fn"
# 1:4 IDENTIFIER "main"
# 1:8 LPAREN "("
# 1:9 RPAREN ")"
# 1:10 LBRACE "{"
# 2:5 KW_INT "int"
# 2:9 IDENTIFIER "counter"
# 2:16 ASSIGN "="
# 2:18 INT_LITERAL "42" 42
# 2:20 SEMICOLON ";"
# 3:1 RBRACE "}"
# 4:1 END_OF_FILE ""
```

### Пример исходного файла

Создайте файл `examples/hello.src`:

```
fn main() {
    int counter = 42;
    return counter;
}
```

Запустите лексер и изучите поток токенов.

## Тестирование

### Запуск всех тестов

```bash
# Запуск через test runner
python tests/test_runner.py

# Или с использованием pytest
pytest tests/ -v

# Запуск с отчётом о покрытии
pytest tests/ --cov=src --cov-report=term
```

### Набор тестов

Тестовый набор включает:

**Корректные тесты (20+):**
- Отдельные типы токенов (ключевые слова, идентификаторы, литералы, операторы, разделители)
- Граничные случаи (мин/макс целые числа, идентификаторы максимальной длины)
- Границы токенов и обработка пробелов
- Смешанные последовательности токенов
- Различные вариации комментариев (однострочные, многострочные, вложенные)
- Разные окончания строк (Unix \n, Windows \r\n)

**Некорректные тесты (10+):**
- Недопустимые символы
- Незавершённые строки
- Незавершённые комментарии
- Неправильно оформленные числа
- Неверные идентификаторы

### Формат вывода тестов

Каждый тест генерирует вывод в следующем формате:
```
СТРОКА:СТОЛБЕЦ ТИП_ТОКЕНА "ЛЕКСЕМА" [ЗНАЧЕНИЕ_ЛИТЕРАЛА]
```

Для некорректных тестов выводятся сообщения об ошибках:
```
Ошибка в СТРОКА:СТОЛБЕЦ: Описание ошибки
```

## Детали реализации

### Компоненты лексера

- **token.py**: Определяет класс Token и перечисление TokenType с более чем 40 категориями токенов
- **scanner.py**: Основная реализация сканера с отслеживанием позиции и поддержкой просмотра вперёд
- **errors.py**: Обработка ошибок с механизмами восстановления и понятными сообщениями

### Ключевые особенности

- **Типы токенов**: Классификация на основе перечислений с полным набором категорий
- **Отслеживание позиции**: Точное отслеживание строк и столбцов (нумерация с 1)
- **Извлечение литералов**: Типизированные значения для целых чисел, чисел с плавающей точкой, строк, булевых значений
- **Восстановление после ошибок**: Пропуск неверных токенов и продолжение сканирования
- **Просмотр вперёд**: Метод `peek_token()` для просмотра следующего токена без его потребления
- **Эффективность памяти**: Одно проход сканирования с минимальным копированием

### Производительность

- Временная сложность: O(n), где n - длина входных данных
- Использование памяти: Пропорционально размеру максимального токена (не всего входа)
- Обработка файлов до 1 МБ

### Обработка ошибок

Лексер предоставляет понятные сообщения об ошибках:
- Недопустимые символы с указанием позиции
- Незавершённые строки
- Незавершённые комментарии
- Неправильно оформленные литералы

## Справочник по API

### Класс Scanner

```python
from src.lexer.scanner import Scanner

# Инициализация сканера с исходным кодом
scanner = Scanner("int x = 42;")

# Получение следующего токена
token = scanner.next_token()

# Просмотр следующего токена без потребления
next_token = scanner.peek_token()

# Проверка достижения конца файла
if scanner.is_at_end():
    print("Конец ввода")

# Получение текущей позиции
line = scanner.get_line()
column = scanner.get_column()
```

### Класс Token

```python
from src.lexer.token import Token, TokenType

# Свойства токена
token.type      # TokenType (перечисление)
token.lexeme    # Исходная строка
token.line      # Номер строки (нумерация с 1)
token.column    # Номер столбца (нумерация с 1)
token.literal   # Типизированное значение (если применимо)
```
